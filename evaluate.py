"""
EDA-ResNet50 è¯„ä¼°è„šæœ¬

å®Œæ•´çš„çš®è‚¤ç™Œåˆ†ç±»æ¨¡å‹è¯„ä¼°å·¥å…·ï¼ŒåŒ…å«è®ºæ–‡æ‰€éœ€çš„æ‰€æœ‰æŒ‡æ ‡ï¼š
- å‡†ç¡®ç‡ (Accuracy)
- æ•æ„Ÿæ€§ (Sensitivity/Recall)
- ç‰¹å¼‚æ€§ (Specificity)
- ç²¾ç¡®ç‡ (Precision)
- F1åˆ†æ•°
- æ··æ·†çŸ©é˜µ
- ROCæ›²çº¿å’ŒAUC
- PRæ›²çº¿
- åˆ†ç±»æŠ¥å‘Š

Usage:
    python evaluate.py --model-path path/to/model.h5 --data-dir path/to/data
"""

import os
import sys
import argparse
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import json
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

# TensorFlow/Keras imports
import tensorflow as tf
from tensorflow.keras.models import load_model
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_auc_score,
    roc_curve, precision_recall_curve, auc
)
from sklearn.preprocessing import label_binarize

# Add src to path for both package and direct invocation
SRC_ROOT = os.path.join(os.path.dirname(os.path.abspath(__file__)), "src")
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
for path in {SRC_ROOT, PROJECT_ROOT}:
    if path not in sys.path:
        sys.path.append(path)

from data.dataset import SkinCancerDataset
from models.eda_resnet50 import (
    create_eda_resnet50,
    compile_eda_resnet50,
    EDAResNet50,
    EDAResNet50Alternative
)
from models.mfr_module import MFRModule, Swish as MFRSwish
from models.efficient_module import EfficientModule, EfficientModuleSimplified
from models.da_module import DualAttentionModule, Swish as DASwish
from models.backbone import ResNet50FeatureExtractor, ResNet50Backbone
from training.metrics import ArgmaxRecallMetric
CUSTOM_OBJECTS = {
    'EDAResNet50': EDAResNet50,
    'EDAResNet50Alternative': EDAResNet50Alternative,
    'MFRModule': MFRModule,
    'MFRSwish': MFRSwish,
    'EfficientModule': EfficientModule,
    'EfficientModuleSimplified': EfficientModuleSimplified,
    'DualAttentionModule': DualAttentionModule,
    'DASwish': DASwish,
    'ResNet50FeatureExtractor': ResNet50FeatureExtractor,
    'ResNet50Backbone': ResNet50Backbone,
    'ArgmaxRecallMetric': ArgmaxRecallMetric
}

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class EDAEvaluator:
    """EDA-ResNet50 æ¨¡å‹è¯„ä¼°å™¨"""

    def __init__(self, model_path: str, data_dir: str, output_dir: str = None):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨

        Args:
            model_path: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„
            data_dir: æ•°æ®é›†ç›®å½•
            output_dir: è¯„ä¼°ç»“æœè¾“å‡ºç›®å½•
        """
        self.model_path = model_path
        self.data_dir = data_dir
        self.output_dir = output_dir or os.path.join(os.path.dirname(model_path), 'evaluation')

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.output_dir, exist_ok=True)

        # è®ºæ–‡ç›®æ ‡æŒ‡æ ‡
        self.paper_targets = {
            'accuracy': 0.9318,
            'sensitivity': 0.94,
            'specificity': 0.925
        }

        # ç±»åˆ«ä¿¡æ¯
        self.class_names = ['benign', 'malignant']
        self.class_mapping = {'benign': 0, 'malignant': 1}

        # åˆå§‹åŒ–æ•°æ®å’Œæ¨¡å‹
        self._setup_data()
        self._load_model()

        print(f"EDA-ResNet50 è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"æ¨¡å‹è·¯å¾„: {model_path}")
        print(f"æ•°æ®è·¯å¾„: {data_dir}")
        print(f"è¾“å‡ºè·¯å¾„: {self.output_dir}")

    def _setup_data(self):
        """è®¾ç½®æ•°æ®åŠ è½½å™¨"""
        print("è®¾ç½®æ•°æ®åŠ è½½å™¨...")

        # åˆ›å»ºæ•°æ®é›†å®ä¾‹
        self.dataset = SkinCancerDataset(
            data_dir=self.data_dir,
            image_size=(224, 224),
            batch_size=32,  # è¯„ä¼°æ—¶å¯ä»¥ç”¨è¾ƒå¤§çš„batch size
            shuffle=False
        )

        # åˆ›å»ºæµ‹è¯•æ•°æ®ç”Ÿæˆå™¨
        self.test_generator = self.dataset.create_test_generator()

        print(f"æµ‹è¯•æ•°æ®é›†: {self.test_generator.samples} æ ·æœ¬")
        print(f"æ‰¹æ¬¡æ•°é‡: {len(self.test_generator)}")

    def _load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        print(f"åŠ è½½æ¨¡å‹: {self.model_path}")

        full_model_candidate = (
            os.path.isdir(self.model_path) or
            self.model_path.endswith('.keras')
        )

        if full_model_candidate:
            try:
                # æ–¹å¼1: ç›´æ¥åŠ è½½å®Œæ•´æ¨¡å‹
                self.model = load_model(
                    self.model_path,
                    custom_objects=CUSTOM_OBJECTS,
                    compile=False
                )
                compile_eda_resnet50(self.model)
                print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ (å®Œæ•´æ¨¡å‹)")
                return
            except Exception as e:
                print(f"å®Œæ•´æ¨¡å‹åŠ è½½å¤±è´¥: {e}")

        # æ–¹å¼2: é‡æ–°æ„å»ºæ¨¡å‹å¹¶åŠ è½½æƒé‡
        try:
            print("å°è¯•é‡æ–°æ„å»ºæ¨¡å‹å¹¶åŠ è½½æƒé‡...")
            self.model = create_eda_resnet50(num_classes=2)
            compile_eda_resnet50(self.model)
            self.model.load_weights(self.model_path)
            print("âœ“ æ¨¡å‹æ„å»ºå’Œæƒé‡åŠ è½½æˆåŠŸ")
        except Exception as e2:
            print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e2}")
            raise

    def predict(self) -> Tuple[np.ndarray, np.ndarray]:
        """
        æ¨¡å‹é¢„æµ‹

        Returns:
            y_true: çœŸå®æ ‡ç­¾
            y_pred: é¢„æµ‹æ¦‚ç‡
            y_pred_classes: é¢„æµ‹ç±»åˆ«
        """
        print("å¼€å§‹æ¨¡å‹é¢„æµ‹...")

        # é‡ç½®ç”Ÿæˆå™¨
        self.test_generator.reset()

        # è·å–é¢„æµ‹ç»“æœ
        steps = len(self.test_generator)
        predictions = self.model.predict(
            self.test_generator,
            steps=steps,
            verbose=1
        )

        # è·å–çœŸå®æ ‡ç­¾
        self.test_generator.reset()
        y_true = []
        for i in range(steps):
            _, batch_labels = next(self.test_generator)
            y_true.extend(np.argmax(batch_labels, axis=1))
        y_true = np.array(y_true)

        # å¤„ç†é¢„æµ‹ç»“æœ
        y_pred = predictions[:len(y_true)]  # ç¡®ä¿é•¿åº¦åŒ¹é…
        y_pred_classes = np.argmax(y_pred, axis=1)

        print(f"é¢„æµ‹å®Œæˆ: {len(y_true)} ä¸ªæ ·æœ¬")
        print(f"é¢„æµ‹æ¦‚ç‡å½¢çŠ¶: {y_pred.shape}")
        print(f"é¢„æµ‹ç±»åˆ«å½¢çŠ¶: {y_pred_classes.shape}")

        return y_true, y_pred, y_pred_classes

    def calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray, y_pred_classes: np.ndarray) -> Dict:
        """
        è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡

        Args:
            y_true: çœŸå®æ ‡ç­¾
            y_pred: é¢„æµ‹æ¦‚ç‡
            y_pred_classes: é¢„æµ‹ç±»åˆ«

        Returns:
            åŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„å­—å…¸
        """
        print("è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")

        # åŸºæœ¬æŒ‡æ ‡
        accuracy = accuracy_score(y_true, y_pred_classes)
        precision = precision_score(y_true, y_pred_classes, average='binary')
        recall = recall_score(y_true, y_pred_classes, average='binary')  # æ•æ„Ÿæ€§
        f1 = f1_score(y_true, y_pred_classes, average='binary')

        # è®¡ç®—ç‰¹å¼‚æ€§
        cm = confusion_matrix(y_true, y_pred_classes)
        if cm.shape == (2, 2):
            tn, fp, fn, tp = cm.ravel()
            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        else:
            specificity = 0

        # å¤šåˆ†ç±»æŒ‡æ ‡ï¼ˆä¸ºæ‰©å±•æ€§å‡†å¤‡ï¼‰
        precision_macro = precision_score(y_true, y_pred_classes, average='macro')
        recall_macro = recall_score(y_true, y_pred_classes, average='macro')
        f1_macro = f1_score(y_true, y_pred_classes, average='macro')

        # AUCæŒ‡æ ‡
        try:
            if len(np.unique(y_true)) == 2:
                # äºŒåˆ†ç±»AUC
                auc_roc = roc_auc_score(y_true, y_pred[:, 1])
                auc_pr = average_precision_score(y_true, y_pred[:, 1])
            else:
                # å¤šåˆ†ç±»AUC
                y_true_bin = label_binarize(y_true, classes=list(range(len(self.class_names))))
                auc_roc = roc_auc_score(y_true_bin, y_pred, average='macro', multi_class='ovr')
                auc_pr = average_precision_score(y_true_bin, y_pred, average='macro')
        except:
            auc_roc = 0
            auc_pr = 0

        metrics = {
            # åŸºæœ¬æŒ‡æ ‡
            'accuracy': accuracy,
            'precision': precision,
            'sensitivity': recall,  # æ•æ„Ÿæ€§ = å¬å›ç‡
            'specificity': specificity,
            'f1_score': f1,

            # å®å¹³å‡æŒ‡æ ‡
            'precision_macro': precision_macro,
            'recall_macro': recall_macro,
            'f1_macro': f1_macro,

            # AUCæŒ‡æ ‡
            'auc_roc': auc_roc,
            'auc_pr': auc_pr,

            # æ··æ·†çŸ©é˜µ
            'confusion_matrix': cm.tolist(),

            # æ ·æœ¬ç»Ÿè®¡
            'total_samples': len(y_true),
            'correct_predictions': int(np.sum(y_true == y_pred_classes)),
            'wrong_predictions': int(np.sum(y_true != y_pred_classes))
        }

        return metrics

    def print_metrics(self, metrics: Dict):
        """æ‰“å°è¯„ä¼°æŒ‡æ ‡"""
        print("\n" + "="*60)
        print("ğŸ¯ EDA-ResNet50 æ¨¡å‹è¯„ä¼°ç»“æœ")
        print("="*60)

        # åŸºæœ¬æŒ‡æ ‡
        print(f"\nğŸ“Š åŸºæœ¬åˆ†ç±»æŒ‡æ ‡:")
        print(f"  å‡†ç¡®ç‡ (Accuracy):     {metrics['accuracy']:.4f}")
        print(f"  ç²¾ç¡®ç‡ (Precision):    {metrics['precision']:.4f}")
        print(f"  æ•æ„Ÿæ€§ (Sensitivity):  {metrics['sensitivity']:.4f}")
        print(f"  ç‰¹å¼‚æ€§ (Specificity):  {metrics['specificity']:.4f}")
        print(f"  F1åˆ†æ•° (F1-Score):     {metrics['f1_score']:.4f}")

        # å®å¹³å‡æŒ‡æ ‡
        print(f"\nğŸ“ˆ å®å¹³å‡æŒ‡æ ‡:")
        print(f"  å®å¹³å‡ç²¾ç¡®ç‡:          {metrics['precision_macro']:.4f}")
        print(f"  å®å¹³å‡å¬å›ç‡:          {metrics['recall_macro']:.4f}")
        print(f"  å®å¹³å‡F1åˆ†æ•°:          {metrics['f1_macro']:.4f}")

        # AUCæŒ‡æ ‡
        print(f"\nğŸ“‰ AUCæŒ‡æ ‡:")
        print(f"  ROC AUC:               {metrics['auc_roc']:.4f}")
        print(f"  PR AUC:                {metrics['auc_pr']:.4f}")

        # æ ·æœ¬ç»Ÿè®¡
        print(f"\nğŸ“Š æ ·æœ¬ç»Ÿè®¡:")
        print(f"  æ€»æ ·æœ¬æ•°:              {metrics['total_samples']}")
        print(f"  æ­£ç¡®é¢„æµ‹:              {metrics['correct_predictions']}")
        print(f"  é”™è¯¯é¢„æµ‹:              {metrics['wrong_predictions']}")

        # ä¸è®ºæ–‡ç›®æ ‡å¯¹æ¯”
        print(f"\nğŸ¯ ä¸è®ºæ–‡ç›®æ ‡å¯¹æ¯”:")
        paper_diff_accuracy = metrics['accuracy'] - self.paper_targets['accuracy']
        paper_diff_sensitivity = metrics['sensitivity'] - self.paper_targets['sensitivity']
        paper_diff_specificity = metrics['specificity'] - self.paper_targets['specificity']

        print(f"  å‡†ç¡®ç‡: {metrics['accuracy']:.4f} (ç›®æ ‡: {self.paper_targets['accuracy']:.4f}, å·®å¼‚: {paper_diff_accuracy:+.4f}) {'âœ“' if metrics['accuracy'] >= self.paper_targets['accuracy'] else 'âœ—'}")
        print(f"  æ•æ„Ÿæ€§: {metrics['sensitivity']:.4f} (ç›®æ ‡: {self.paper_targets['sensitivity']:.4f}, å·®å¼‚: {paper_diff_sensitivity:+.4f}) {'âœ“' if metrics['sensitivity'] >= self.paper_targets['sensitivity'] else 'âœ—'}")
        print(f"  ç‰¹å¼‚æ€§: {metrics['specificity']:.4f} (ç›®æ ‡: {self.paper_targets['specificity']:.4f}, å·®å¼‚: {paper_diff_specificity:+.4f}) {'âœ“' if metrics['specificity'] >= self.paper_targets['specificity'] else 'âœ—'}")

        # æ€»ä½“è¯„ä»·
        all_targets_met = (
            metrics['accuracy'] >= self.paper_targets['accuracy'] and
            metrics['sensitivity'] >= self.paper_targets['sensitivity'] and
            metrics['specificity'] >= self.paper_targets['specificity']
        )

        print(f"\nğŸ† è®ºæ–‡å¤ç°çŠ¶æ€: {'âœ“ æˆåŠŸ' if all_targets_met else 'âœ— éƒ¨åˆ†æˆåŠŸ'}")

        # æ··æ·†çŸ©é˜µ
        cm = np.array(metrics['confusion_matrix'])
        if cm.shape == (2, 2):
            tn, fp, fn, tp = cm.ravel()
            print(f"\nğŸ” æ··æ·†çŸ©é˜µè¯¦æƒ…:")
            print(f"  çœŸè´Ÿä¾‹ (TN): {tn}")
            print(f"  å‡æ­£ä¾‹ (FP): {fp}")
            print(f"  å‡è´Ÿä¾‹ (FN): {fn}")
            print(f"  çœŸæ­£ä¾‹ (TP): {tp}")

        print("\n" + "="*60)

    def plot_confusion_matrix(self, y_true: np.ndarray, y_pred_classes: np.ndarray):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        cm = confusion_matrix(y_true, y_pred_classes)

        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=self.class_names,
                   yticklabels=self.class_names)
        plt.title('EDA-ResNet50 Confusion Matrix', fontsize=16, fontweight='bold')
        plt.xlabel('Predicted Label', fontsize=12)
        plt.ylabel('True Label', fontsize=12)

        # ä¿å­˜å›¾ç‰‡
        save_path = os.path.join(self.output_dir, 'confusion_matrix.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"æ··æ·†çŸ©é˜µå·²ä¿å­˜: {save_path}")
        plt.show()

    def plot_roc_curve(self, y_true: np.ndarray, y_pred: np.ndarray):
        """ç»˜åˆ¶ROCæ›²çº¿"""
        if len(np.unique(y_true)) != 2:
            print("ROCæ›²çº¿ä»…æ”¯æŒäºŒåˆ†ç±»ï¼Œè·³è¿‡ç»˜åˆ¶")
            return

        fpr, tpr, _ = roc_curve(y_true, y_pred[:, 1])
        roc_auc = auc(fpr, tpr)

        plt.figure(figsize=(8, 6))
        plt.plot(fpr, tpr, color='darkorange', lw=2,
                label=f'ROC Curve (AUC = {roc_auc:.4f})')
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--',
                label='Random Classifier')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate (FPR)', fontsize=12)
        plt.ylabel('True Positive Rate (TPR)', fontsize=12)
        plt.title('EDA-ResNet50 ROC Curve', fontsize=16, fontweight='bold')
        plt.legend(loc="lower right")
        plt.grid(True, alpha=0.3)

        # ä¿å­˜å›¾ç‰‡
        save_path = os.path.join(self.output_dir, 'roc_curve.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ROCæ›²çº¿å·²ä¿å­˜: {save_path}")
        plt.show()

    def plot_precision_recall_curve(self, y_true: np.ndarray, y_pred: np.ndarray):
        """ç»˜åˆ¶PRæ›²çº¿"""
        if len(np.unique(y_true)) != 2:
            print("PRæ›²çº¿ä»…æ”¯æŒäºŒåˆ†ç±»ï¼Œè·³è¿‡ç»˜åˆ¶")
            return

        precision, recall, _ = precision_recall_curve(y_true, y_pred[:, 1])
        pr_auc = auc(recall, precision)

        plt.figure(figsize=(8, 6))
        plt.plot(recall, precision, color='blue', lw=2,
                label=f'PR Curve (AUC = {pr_auc:.4f})')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('Recall', fontsize=12)
        plt.ylabel('Precision', fontsize=12)
        plt.title('EDA-ResNet50 Precision-Recall Curve', fontsize=16, fontweight='bold')
        plt.legend(loc="lower left")
        plt.grid(True, alpha=0.3)

        # ä¿å­˜å›¾ç‰‡
        save_path = os.path.join(self.output_dir, 'pr_curve.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"PRæ›²çº¿å·²ä¿å­˜: {save_path}")
        plt.show()

    def save_results(self, metrics: Dict, y_true: np.ndarray, y_pred: np.ndarray, y_pred_classes: np.ndarray):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        results = {
            'evaluation_time': datetime.now().isoformat(),
            'model_path': self.model_path,
            'data_path': self.data_dir,
            'paper_targets': self.paper_targets,
            'metrics': metrics,
            'class_names': self.class_names,
            'detailed_classification_report': classification_report(
                y_true, y_pred_classes,
                target_names=self.class_names,
                output_dict=True
            )
        }

        # ä¿å­˜JSONç»“æœ
        results_path = os.path.join(self.output_dir, 'evaluation_results.json')
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        print(f"è¯„ä¼°ç»“æœå·²ä¿å­˜: {results_path}")

        # ä¿å­˜é¢„æµ‹ç»“æœ
        predictions_path = os.path.join(self.output_dir, 'predictions.npz')
        np.savez(predictions_path,
                y_true=y_true,
                y_pred=y_pred,
                y_pred_classes=y_pred_classes)
        print(f"é¢„æµ‹ç»“æœå·²ä¿å­˜: {predictions_path}")

    def evaluate(self, plot_curves: bool = True, save_results: bool = True):
        """
        æ‰§è¡Œå®Œæ•´è¯„ä¼°

        Args:
            plot_curves: æ˜¯å¦ç»˜åˆ¶æ›²çº¿
            save_results: æ˜¯å¦ä¿å­˜ç»“æœ
        """
        print("ğŸš€ å¼€å§‹EDA-ResNet50æ¨¡å‹è¯„ä¼°...")
        print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        # æ¨¡å‹é¢„æµ‹
        y_true, y_pred, y_pred_classes = self.predict()

        # è®¡ç®—æŒ‡æ ‡
        metrics = self.calculate_metrics(y_true, y_pred, y_pred_classes)

        # æ‰“å°ç»“æœ
        self.print_metrics(metrics)

        # ç»˜åˆ¶å›¾è¡¨
        if plot_curves:
            print("\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
            self.plot_confusion_matrix(y_true, y_pred_classes)
            self.plot_roc_curve(y_true, y_pred)
            self.plot_precision_recall_curve(y_true, y_pred)

        # ä¿å­˜ç»“æœ
        if save_results:
            print("\nğŸ’¾ ä¿å­˜è¯„ä¼°ç»“æœ...")
            self.save_results(metrics, y_true, y_pred, y_pred_classes)

        print(f"\nâœ… è¯„ä¼°å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {self.output_dir}")
        return metrics


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='EDA-ResNet50 æ¨¡å‹è¯„ä¼°')

    parser.add_argument('--model-path', type=str, required=True,
                       help='è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„ (.h5 æ–‡ä»¶)')

    parser.add_argument('--data-dir', type=str,
                       default="/root/EDA-ResNet50/training_data_Skin Cancer_Malignant_vs_Benign",
                       help='æ•°æ®é›†ç›®å½•è·¯å¾„')

    parser.add_argument('--output-dir', type=str, default=None,
                       help='è¯„ä¼°ç»“æœè¾“å‡ºç›®å½•')

    parser.add_argument('--no-plots', action='store_true',
                       help='ä¸ç»˜åˆ¶å¯è§†åŒ–å›¾è¡¨')

    parser.add_argument('--no-save', action='store_true',
                       help='ä¸ä¿å­˜è¯„ä¼°ç»“æœ')

    args = parser.parse_args()

    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.model_path):
        print(f"âŒ é”™è¯¯: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model_path}")
        sys.exit(1)

    try:
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = EDAEvaluator(
            model_path=args.model_path,
            data_dir=args.data_dir,
            output_dir=args.output_dir
        )

        # æ‰§è¡Œè¯„ä¼°
        metrics = evaluator.evaluate(
            plot_curves=not args.no_plots,
            save_results=not args.no_save
        )

        print("\nğŸ‰ è¯„ä¼°è„šæœ¬æ‰§è¡ŒæˆåŠŸ!")

    except Exception as e:
        print(f"âŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()